# #학습조직 #2, 케라스를 이용한 인공 신경망 소개

### CH10, Articifial Neural Network

인공신경망이란 동물의 뇌의 생물학적 뉴런이 복잡한 계산을 하기 위해 어떻게 상호작용 하는가에서 아이디어를 얻어 이를 기반으로 계산 모델을 생성한 것이라고 할 수 있음. 인공신경망은 1990년대 까지만 해도 svm 과 같은 머신러닝 알고리즘에 밀려 빛을 보지 못했으나, 2000년대에 접어 인공신경망을 여러개 겹처 생성한 딥러닝 모델이 2012년  ILSVRC2012 대회에서 우승하면서 다시금 주목받게 되었음.

인공신경망, 즉 딥러닝이 주목받게 된 이유에는 위에서 설명한 것 이외에도 데이터 양의 증가, 컴퓨터의 성능 발달 등의 요인이 복합적으로 작용하며 각광을 받게 되었음.

이러한 인공신경망의 기원이 되는 알고리즘이 있는데, 그것이 바로 이후 논의할 퍼셉트론임.

1. ==퍼셉트론==

   ![img](https://github.com/minsuk-sung/Hands-On-MachineLearning/raw/master/Chap10-%EC%9D%B8%EA%B3%B5%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%86%8C%EA%B0%9C/images/perceptron02.png)

   퍼셉트론이란, 위 그림과 같이 입력과 출력이 어떤 숫자로 주어졌을 때 각각의 입력에 각각 고유한 가중치가 곱해지게 되고, 그런 다음 계산된 합에 계단함수를 적용하여 결과  h를 출력하는 것을 의미함.  

   Frank Rosenblatt가 제안한 퍼셉트론의 학습 알고리즘은 **헤브의 규칙**(Hebb's rule)로 부터 영감을 받았는데, 헤브의 규칙이란 두 뉴런이 동일한 출력을 낼 때마다 이 둘 사이의 연결 가중치가 증가하는 것을 의미함. 즉, 퍼셉트론은 네트워크가 만드는 에러를 반영하도록 학습되며, 잘못된 출력을 만드는 연결은 올바른 출력을 만들 수 있도록 가중치를 조정 하게 됨. 

   Frank Rosenblatt는 학습 데이터가 선형적으로 구분될 수 있으면, 퍼셉트론은 정답에 수렴한다는 것을 보였는데 이를 **퍼셉트론 수렴 이론**(Perceptron convergence theorem)이라고 함. 하지만, 출력 뉴런의 decision boundary는 선형(결합)이므로 퍼셉트론은 복잡한 패턴 (e.g. 비선형)은 학습하지 못한다는 단점이 있음. 

   이러한 퍼셉트론의 단점 때문에 신경망은 다시 암흑기를 맞아 드나 싶엇으나, 퍼셉트론을 여러 층 쌓아 만든 다층 퍼셉트론을 통해 비선형 문제를 해결 할 수 있게 되었음. 

2. ==다중 퍼셉트론==

   다층 퍼셉트론(MLP)은 아래의 그림과 같이 입력층, **은닉층**(hidden layer)이라 부르는 하나 이상의 TLU 층과 마지막으로 **출력층**(output layer)으로 구성됨. 인공 신경망의 은닉층이 2개 이상일 때, **심층 신경망**(**DNN**, Deep Neural Network)라 하고 이를 학습하여 모델을 만드는 것을 우리가 익히 들어온 **딥러닝**(Deep-Learning) 이라고 함.

![img](https://github.com/minsuk-sung/Hands-On-MachineLearning/raw/master/Chap10-%EC%9D%B8%EA%B3%B5%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%86%8C%EA%B0%9C/images/mlp.png)

​		여러층을 쌓은 MLP를 통해 XOR(non-linear한 구조 예측 불가) 문제를 해결 했지만, 층이 깊어질 수록 증가하는 가중		치 매개변수의 수로 인해 다층 퍼셉트론을 학습시키기에는 오랜 시간이 걸리는 문제가 발생했다. 하지만, 1986년 **역전		파(backpropogation)** 알고리즘이 등장하면서 계산량을 획기적으로 줄일 수 있게 되었음. 

3. ==역전파, 활성화 함수==

   역전파 과정을 간단하게 설명하면 다음과 같음.

   1. 먼저, 각 학습 데이터 샘플을 네트워크에 입력으로 넣어주고 출력층까지 각 층의 뉴런 마다 출력을 계산한다. 이를 **순전파**(forward propagation)이라고 한다.
   2. 그 다음 네트워크의 마지막 출력층에 대한 결과(예측값)와 실제값과의 차이, 즉 오차(error)를 계산하는데, 손실함수(loss function)를 이용하여 계산한다.
   3. 그리고 이 오차를 역방향으로 흘러 보내면서, 각 출력 뉴런의 오차에 마지막 입력 뉴런이 얼마나 기여했는지 측정한다. 이말을 쉽게 설명하면, **각 뉴런의 입력값에 대한 손실함수의 편미분, 그래디언트(gradient)을 계산**하는 것을 말한다.
   4. 3번과 같은 방법을 입력층에 도달할 때까지 계속 반복해서 역방향으로 흘러 보낸다.
   5. 마지막으로, 계산한 그래디언트를 네트워크의 모든 가중치 매개변수에 반영해주는 **경사 하강법 단계**를 수행한다.

   따라서, 먼저 순전파를 통해 네트워크가 예측을 출력(출력층의 출력 결과)하고, 이를 손실함수를 이용해 오차를 계산한 뒤, 역방향으로 거슬러 올라가면서 각 뉴런의 입력값에 대한 손실함수의 편미분을 계산하고(backpropagation), 이를 경사 하강법을 이용해 가중치를 조정하는 방법이 역전파 알고리즘임. 

   ![image-20210519140627047](/Users/jueunhye/Library/Application Support/typora-user-images/image-20210519140627047.png)

   (간단한 역전파 과정 - 출처 : coursera deep learning 강의)

   역전파 알고리즘이 잘 동작하기 위해서 다층 퍼셉트론(MLP)의 구조가 계단 함수에서 시그모이드 함수로 바뀌게 되었는데, 이는 편미분을 계산하기 위함임. (계단함수는 기울기가 없는 직선이므로 그래디언트 계산이 불가함)

   ![img](https://github.com/minsuk-sung/Hands-On-MachineLearning/raw/master/Chap10-%EC%9D%B8%EA%B3%B5%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%86%8C%EA%B0%9C/images/activation02.png)

   시그모이드 이외에도 위와 같은 다양한 활성화 함수들을 사용 할 수 있는데, 그래디언트 로스 문제 방지를 위해 주로 ReLU 함수를 사용하는 편임. 

