# 학습조직 #3, 심층 신경망 훈련하기

## 11.1 그래디언트 소실과 폭주 문제

이전 세션에서 살펴 보앗듯이,  역전파 알고리즘은 출력층(output layer)에서 입력층(input layer)로 오차 그래디언트(gradient)를 흘려 보내면서 각 뉴런의 입력값에 대한 손실함수의 그래디언트를 계산함. 이후 경사 하강법(gradient descent)단계에서 이 그래디언트를 사용하여 각 파라미터 값(가중치 값들)을 수정하게 됨. 

하지만, 아래의 그림과 같이 깊이가 깊은 심층신경망에서는 역전파 알고리즘이 입력층으로 전달됨에 따라 그래디언트가 점점 작아져 결국 가중치 매개변수가 업데이트 되지 않는 경우가 발생하게 되는데,  이러한 문제를 **그래디언트 소실**(vanishing gradient)라고 함.

![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0exms5v7j30i90bcafp.jpg)

그래디언트 소실과는 반대로 역전파에서 그래디언트가 점점 커져 입력층으로 갈수록 가중치 매개변수가 기하급수적으로 커지게 되는 경우가 있는데 이를 **그래디언트 폭주**(exploding gradient)라고 하며, 이 경우에는 발산(diverse)하게되어 학습이 제대로 이루어지지 않게 됨. 

이 같은 그래디언트 소실 문제의 원인은 바로 활성화 함수로 가장 흔하게 사용되던 로지스틱 함수(시그모이드)와 그 당시 가장 인기 있던 가중치 초기화 방법(평균이 0이고 분산이 1인 정규분포) 의 조합 때문이엇음. 



### 1. 수렴하지 않는 활성화 함수

![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0f6r4kl4j30bj08bq3a.jpg)

위는 시그모이드 함수의 그래프인데, 시그모이드 함수는 다음과 같은 특징을 지니고 있음. 

- 입력 신호의 총합을 0에서 1사이의 값으로 바꿔줌.

- 입력 신호의 값이 커질수록(작아질수록) 뉴런의 활성화률(firing rate)이 1(작아질 경우 0 )로 수렴(saturation)함.

시그모이드 함수를 살펴보면 알 수 있다시피, input의 값이 커지거나 작아짐에 따라 함수를 통과한 output의 값이 1 또는 0으로 수렴하게 된다는 사실을 알 수 있는데, 이러한 시그모이드의 특성 때문에 해당 함수는 2가지의 단점이 존재하게 됨.

- 입력의 절대값이 크게 되면 0이나 1로 수렴하게 되는데, 이러한 뉴런들은 **그래디언트를 소멸(kill) 시켜 버림.** 그 이유는 수렴된 뉴런의 그래디언트 값은 0이기 때문에 역전파에서 0이 곱해지기 때문. 따라서, 역전파가 진행됨에 따라 아래 층(layer)에는 아무것도 전달되지 않는 현상이 발생하게 됨. 
- **원점 중심이 아님(Not zero-centered)**. 따라서, 평균이 0이 아니라 0.5이며, 시그모이드 함수는 항상 양수를 출력하기 때문에 출력의 가중치 합이 입력의 가중치 합보다 커질 가능성이 높은데,  이것을 편향 이동(bias shift)이라 함.  이러한 이유로 **각 레이어를 지날 때마다 분산이 계속 커져** 가장 높은 레이어에서는 활성화 함수의 출력이 0이나 1로 수렴하게 되어 그래디언트 소실 문제가 발생하게 되는 것임. 

이러한 시그모이드의 특징 때문에 그래디언트 소실 문제가 발생하게 되는데, 이같은 문제를 해결 한 것이 현재 가장 흔하게 쓰이는 활성화 함수 중 하나인 **ReLu** 임



![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0ffj69oxj30a005yjrl.jpg)

그러나 Relu 역시 단점을 가지고 있는데, 흔히 dead ReLu라고 부르는, 모델이 학습하는 동안 **일부 뉴런이 0만을 출력하여 활성화 되지 않는 문제** 임

특히 학습률(learning rate)이 클 경우, 모델의 뉴런이 절반정도가 죽어 있기도(뉴런이 0만 출력) 함. 이렇게 뉴런이 0만을 출력하는 이유는 학습이 진행되면서 뉴런의 **가중치가 업데이트 되면서 가중치 합이 음수가 되는 순간** ReLU에 의해 그 이후로는 0 값만 출력이 되기 때문인데, 이후 그래디언트는 계속해서 0값만 곱해지기 때문에 그래디언트 소실이 발생하게 됨. 

이러한 문제를 해결하기 위해 사용하는 것이 바로 **leakyReLu와 RReLU, 그리고 2015년에 새로 고안된 ELU라는 활성화 함수임.**

1. LeakyReLU

   ![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0fo5545qj307e066mxh.jpg)

   leakyReLU는 하이퍼파라미터 a가 추가 된 ReLu의 변종인데, 이 a값이 input값이 0 이하에서의 기울기를 절대 0이 되지 않게 만들어 주기 때문에 leakyRelu를 절대 죽지 않게 만들어줌. 

2. RReLU

   RReLU는 a 값을 무작위로 선택하고 테스트 시에는 평균 a값을 사용하는 함수로, 오버피팅을 방지하는 역할도 수행함 

3. ELU

   ![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0fzvxj4tj306q04vjrh.jpg)

   - x < 0일 때 ELU 활성화 함수 출력의 평균이 0(zero mean)에 가까워지기 때문에 편향 이동(bias shift)이 감소하여 그래디언트 소실 문제를 줄여줌.  하이퍼파라미터인 a는 x가 음수일 때 ELU가 수렴할 값을 정의하며 보통 1로 설정.

   - x < 0 이어도 그래디언티가 0이 아니므로 죽은(dead) 뉴런을 생성하지 않음.

   - a = 1일 때 ELU는 x=0에서 급격하게 변하지 않고 모든 구간에서 매끄럽게 변하기 때문에 경사하강법에서 수렴속도가 빠름.

해당 ELU는 그래디언트 소실 문제를 크게 감소시킬 수 있지만, 아예 발생하지 않는다고 보장을 시켜주지는 않음. 이러한 상황에서 그래디언트 소실 문제를 해결하기 위해 추가로 고안된 것이 바로 **배치 정규화임**

별첨) 활성화 함수의 종류들 

![image-20210530181657105](https://tva1.sinaimg.cn/large/008i3skNgy1gr0kobj79kj31440mc17e.jpg)



### 2. 배치 정규화

2015년 Sergety Ioffe와 Christian Szegedy는 ['Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift'](https://arxiv.org/pdf/1502.03167v3.pdf)라는 논문에서 **배치 정규화**(BN, Batch Normalization)를 제안함. 배치 정규화는 각 층의 활성화 함수의 출력값 분포가 골고루 분포되도록 '강제'하는 방법으로, 각 층에서의 활성화 함수 출력값이 정규분포(normal distribution)를 이루도록 하는 방법임. 

즉, 학습하는 동안 이전 레이어에서의 가중치 매개변수가 변함에 따라 활성화 함수 출력값의 분포가 변화하는 **내부 공변량 변화(Internal Covariate Shift) 문제를 줄이는 방법**이 바로 배치 정규화 기법.

배치 정규화는 아래의 그림과 같이 미니배치(mini-batch)의 데이터에서 각 feature(특성)별 평균($\mu$, mean)과 분산($\sigma^{2}$, variance)을 구한 뒤 정규화(normalize) 를 진행하게 됨.

![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0gw1gcjpj30k608d3zj.jpg)

일반적으로 배치 정규화는 아래의 그림과 같이 Fully Connected(FC)나 Convolutional layer 바로 다음, 활성화 함수를 통과하기 전에 배치 정규화(BN)레이어를 삽입하여 주로 사용함.

![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0gwr2p9cj30iu07w3ys.jpg)



Batch Normalization(BN)은 논문에서 실험했던 모든 DNN의 성능이 크게 향상 시켰는데, BN은 다음과 같은 장점들이 있음. 

- tanh나 sigmoid 같은 활성화 함수에 대해 그래디언트 소실(vanishing gradient)문제가 감소함.

- 가중치 초기화에 덜 민감하다. 가중치 초기값에 크게 의존하지 않기 때문에 가중치 초기화 기법에 대해 크게 신경 쓰지 않아도 됨.

- 학습률(learning rate)를 크게 잡아도 gradient descent가 잘 수렴함.

- 오버피팅을 억제함.  BN이 마치 Regularization 역할을 하기 때문에 드롭아웃(Dropout)과 같은 규제기법에 대한 필요성이 감소함. 하지만 BN로 인한 규제는 효과가 크지 않기 때문에 드롭아웃을 함께 사용하는 것이 좋음.

  

## 11.2 사전훈련된 층 재사용하기

### 1. 케라스를 이용한 전이학습

규모가 매우 큰 DNN 모델을 학습 시킬 때 처음부터 새로 학습 시키는 것은 효율성이 매우 떨어짐(시간이 오래 걸리기 때문). 이러한 경우 기존에 학습된 비슷한 DNN모델이 있을 때 이 모델의 하위층(lower layer)을 가져와 재사용하는 것이 학습 속도를 빠르게 할 수 있을 뿐만아니라 학습에 필요한 Training set도 훨씬 적어 효율적임. 이러한 학습된 기존 모델에서의 일부분을 재사용하여 새로운 모델에 사용하는 것을 전이학습(Transfer Learning)이라고 함. 

![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0h2vo3o0j30k70gfgn8.jpg)

전이 학습과 관련된 내용은 이후 실습파일에서 진행하도록 함. (실습 위주)



## 11.3. 고속 옵티마이저

큰 층을 가진 신경망의 속도는 매우 느릴 수 밖에 없는데, 표준적인 경사 하강법 옵티마이저 대신 더 빠른 옵티마이저들을 사용하면 신경망의 학습 속도를 크게 증진 시킬 수 있음. 해당 챕터에서는 다양한 옵티마이저들에 대해 학습을 진행하도록 함. 

1. 모멘텀 최적화

   모멘텀 최적화는 그래디언트를 속도가 아니라 가속도로 사용하는 방식으로, 최적점에 도달 할 때까지 점점 더 빠르게 내려가 **경사 하강법보다 더 빠르게 평편한 지역을 탈출** 할 수 있게끔 하는 알고리즘임. 

   ![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0he1tr8kj30b408pmxp.jpg)

   우리가 흔히 사용하는 경사 하강법의 움직임이 위 사진과 같을때, 

   ![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr0hfjoz97j30b408rt98.jpg)

   모멘텀을 활용하여 매개변수 최적화를 진행 하게 되면 위의 사진처럼 최적의 매개변수를 찾는데까지의 경로가 굉장히 효율적으로 변함. 

   모멘텀을 사용하게 되면 경사 하강법을 사용 할 때보다 튜닝할 파라미터의 갯수가 한 개 더 증가하였다는 단점이 있지만, 보통 모멘텀 0.9정도에서 잘 작동하기 때문에 momentum = 0.9로 두고 주로 사용함. 

   

2. 네스테로프 가속 경사

   네스테로프 가속 경사는 1983년 유리 네스테로프가 제안한 모멘텀 최적화의 변종 중 하나로, 현재 위치 θ가 아니라 모멘텀 방향으로 조금 앞선 θ+βm에서 비용 함수의 그래디언트를 계산하는 알고리즘임. 

   **기본 모멘텀 최적화보다 거의 항상 더 빠르다는 장점**을 가지고 있음. 네스테로프 가속 경사는 원래 위치에서의 그래디언트를 사용하는 것이 아니라 그 방향으로 조금 더 나아가서 측정한 그래디언트를 사용한 방식인데, 일반적으로 모멘터머 벡터가 최적점을 향하는 방향을 가리킬 것이라는 가정 하에 사용됨. 

   ![고속 옵티마이저 - Dev-hwon's blog](https://tva1.sinaimg.cn/large/008i3skNgy1gr0hmowth6j30ix0aegti.jpg)

   

3. AdaGrad

   AdaGrad는 각각의 매개변수에 적응적학습률 (Adaptive Learning rate)을 조정하며 학습을 진행하는 최적화 알고리즘으로, 경사가 완만한 차원보다 가파른 차원에 대해 더 빠르게 감소하게 되어 **전역 최적점 방향으로 더 곧장 가도록 하는 특징**이 있음.

   ![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr1q7lwi36j30b408wgm3.jpg)

   y축 방향으로 처음 기울기가 큰 탓에 갱신 강도가 빨랐다가 점점 약해지면서 작은 움직임으로 최솟값에 도달하는 것을 확인할 수 있음.

   단 AdaGrad는 학습률이 너무 감소되어 전역 최적점에 도달하기 전에 알고리즘이 완전히 멈춘다는 단점 역시 존재하기 때문에, 회귀분석 같은 간단한 알고리즘에는 효과적으로 작동할 수 있으나, 심층 신경망에는 사용을 지양해야함. 이같은 AdaGrad의 단점을 보완 한 것이 RMSProp임. 

   

4. RMSProp

   RMSProp는 과거의 모든 기울기를 균일하게 더하지 않으며, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영하는 알고리즘임.  이를 지수이동평균 Exponential Moving Average, EMA라고 하고 과거 기울기의 반영 규모를 기하급수적으로 감소시키는 특징이 있음. 이러한 특징 때문에 너무 빨리 느려져서 전역 최적점에 수렴하지 못한다는 AdaGrad의 단점을 극복 할 수 있었고 항상 AdaGrad보다 우수한 성능을 보인다는 장점이 있음. 

   

5. Adam

   **Adam은 모멘텀 최적화와 RMSProp의 아이디어를 합친 것**으로, 모멘텀 최적화처럼 지난 그레디언트의 지수 감소 평균을 따르고 RMSProp처럼 지난 그레디언트 제곱의 지수 감소된 평균을 따름. 

   

![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr1qeltxflj30b408tmxm.jpg)

모멘텀과 비슷한 방향으로 움직이나,  모멘텀 방법과 비교해서 공의 좌우 흔들림이 적은 것을 확인할 수 있음. 이는 각각의 매개변수에 Adaptive learning rate를 적용하여 학습을 진행하였기 때문임. 

참고)  각 최적화 기법 비교

![img](https://tva1.sinaimg.cn/large/008i3skNgy1gr1qhqclmqj30b107qq3q.jpg)

