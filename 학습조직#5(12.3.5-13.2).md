# 학습조직#5(12.3.5-13.2)

## 12.3.5 - 12.12.4.2 

### 12.3.5 새로운 사용자 정의 층 생성

- 함수 생성 뒤, lambda 함수에 감싸기

  keras.layers.Lambda(lambda x : tf.exp(x))

- 생성자는 모든 하이퍼파라미터를 매개변수로 받아야 함.
- build 메서드의 역할은 가중치마다 add_weight를 호출하여 층의 변수를 만드는 것.

### 12.3.6 사용자 정의 모델

잔차 블록(Residual block)을 활용하여 반복문이 존재하는 모델을 생성 -> 층이 다른 층을 포함 하고 있는 구조임.

### 12.3.8 자동 미분으로 그레디언트 계산

각 파라미터가 바뀔 때마다 함수의 출력이 얼마나 변하는지 측정하여 도함수의 근삿값을 계산하는 방식

-> 꽤 정확한 그레디언트 값이 도출되지만 근삿값이고 파라미터마다 함수의 출력의 변화량을 측정해야 되기 때문에 대규모 신경망 구조에서는 사용하기 어려움.

-> 텐서플로에서 제공하는 자동 미분 기능을 사용하여 해결

- GradientTape() 함수는 여러 값(일반적으로 모델 파라미터) 에 대한 한 값의 그레디언트를 계산하는데 주로 사용되기 때문에, 개별 그레디언트 값을 계산하기 위해서는 Jacobain() 함수를 사용해야 함.
- 그레디언트의 역전파를 방지해야 할 경우에는 tf.stop_gradient()를 사용.

### 12.3.9 사용자 정의 훈련 반복

텐서플로우가 제공하는 fit()이 마음에 들지 않거나, 원하는 훈련이 있을때 자체적으로 코드를 개발하여 훈련을 진행하는 것.

- Batch Normalization이나 Dropout같이 훈련과 테스트시에 다르게 동작하는 층은 다루지 못함.
- 모델 유지보수가 어렵고 버그가 일어날 가능성이 높음.

### 12.4 텐서플로 함수와 그래프

tf.function() 함수를 사용하여 보통의 함수를 텐서플로 함수로 변환 할 수 있음. 단, 케라스는 대부분의 경우에 파이썬 함수를 자동으로 텐서플로우 함수로 변환해 주기 때문에 사용자가 tf.function()을 직접 사용할 일은 거의 없음.

**텐서플로우 함수**

파이썬 함수의 break, continue, return같은 요소들을 모두 찾은 다음 이것들을 모두 텐써 연산으로 바꿈.

## 13.1-13.1.6

### 13.1.2 데이터 셔플링

경사 하강법의 경우 training 데이터가 독립적이고 동일한 분포를 지니고 있을 때 최고의 성능을 발휘하기 때문에, 모델링을 진행 하기 전 shuffle() 메서드를 활용하여 샘플을 섞어주는 것이 좋음. 

- shuffle() : 원본 데이터셋의 처음 아이템을 buffer_size 개수만큼 추출하여 버퍼에 채운 뒤, 새로운 아이템이 요청되면 버퍼에서 랜덤하게 하나를 꺼내 반환, 이후 원본 데이터셋에서 새로운 데이터를 추출하여 버퍼를 다시 채움.
- 버퍼 크기를 크게 지정하는 것이 좋으나, 메모리 크기를 넘지 않게 주의해야 함.

메모리에 맞지 않는 매우 큰 데이터셋인 경우 일반적으로 먼저 여러 개의 파일로 나누고 텐서플로에서 이 파일들을 병렬로 읽게 처리해야 하는데, 여기에 사용되는 함수가 list_files() 임.

- list_files() 함수를 사용하여 파일 경로를 섞은 데이터셋을 반환, 이후 interleave() 메서드를 호출하여 한번에 다섯 개의 파일을 한 줄 씩 번갈아 읽음. 

  -> 10개의 파일이 있으면 한 번에 10개의 파일을 한 줄씩 읽는것이지 병렬로 동시에 읽는 것은 아님

  -> 병렬로 데이터를 한줄씩 동시에 읽고 싶으면 num_parallel_calls 매게변수에 원하는 스레드를 지정해야 함. 

  





