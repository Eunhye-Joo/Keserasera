# 학습조직#4, 규제, 드롭아웃, CH12 실습

## 11.4 규제를 사용해 과대적합 피하기

규제 부분 내용을 들어가기 전에, 규제를 왜 해야하는지, 과대적합을 왜 방지해야 하는지에 대해 다시 한번 짚고 넘어가도록 함. 

![image-20210620141425609](https://tva1.sinaimg.cn/large/008i3skNgy1gronoj8pvsj31f60sg77f.jpg)

과적합은 우리가 흔히 들어봤던 bias variance에서 variance가 지나치게 높은 경우에 해당됨. bias가 매우 낮아서 예측력은 높지만, 예측값에 대한 분산이 너무 심해 학습에 사용된 데이터셋이 아닌 다른 데이터셋에 대해 예측을 진행하면 예측력이 형편없게 바뀔 수 있는 가능성이 대단히 높은 것임. 이렇듯 과적합이 진행되게 되면 test data에 예측을 진행 할 경우 대부분의 경우에서 예측력이 매우 낮아지게 되고, 우리의 모델 결과를 신뢰할 수 없게 되는것임. 따라서 과적합을 방지하기 위해서 규제, 드랍아웃 등의 방법론을 사용함. 

### 11.4.1 l1 규제, l2규제

L1규제는, 기존의 손실함수에 가중치(W) 의 절대값인 L1노름을 추가하여 규제를 진행하는 방식임. 

활성화 함수로 로지스틱 함수를 사용한다고 했을 때, 로지스틱 손실 함수는 다음과 같고, 

![img](https://tva1.sinaimg.cn/large/008i3skNgy1gronulruafj30kt01y748.jpg)

아래의 식이 바로 L1 규제 term을 추가 한 로지스틱 손실 함수임. 

![img](https://tva1.sinaimg.cn/large/008i3skNgy1gronuvyi8qj30qy03vmx9.jpg)

우리는 하이퍼파라미터 o의 값을 조정하는 것을 통해 규제의 정도를 결정 할 수 있음. o값이 커지면 w의 sum이 작아져 규제의 정도가 강해지게 되며, o값이 작아지면 규제의 정도가 약해지게 됨. 많이 들어봤겠지만, 일반 회귀 모델에서 L1규제를 진행하는 것을 Lasso라고 부름.

L2규제는, 손실함수에 L2노름의 제곱을 추가하여 규제를 진행하는 방식으로, L2규제가 추가된 로지스틱 손실함수의 식은 다음과 같음.

![img](https://tva1.sinaimg.cn/large/008i3skNgy1grony5muqwj30ua03lwen.jpg)

L2규제는 그래디언트 계산에 가중치 값 자체가 포함되기 때문에, 가중치의 부호만 사용하는 L1규제보다는 좀 더 효과적이라고 알려 져 있음. **또한, o값이 커짐에 따라 가중치가 완벽하게 0이 되는 L1규제와 다르게 L2규제는 가중치를 완전히 0으로 만들지는 않음.**회귀 모델에서 L2 규제가 적용된 것을 Ridge모형이라고 함. 

keras에서는 keras.regularizers.l1_l2() 함수를 사용하여 손쉽게 규제를 진행 할 수 있음. 

### 11.4.2 드롭아웃

드롭아웃은 신경망을 훈련하는 과정에서 각 뉴런을 임의로 drop해 버리는 것을 뜻하는데, 매 훈련스텝마다 드롭확률p를 지정하여 임의의 뉴런을 완벽하게 무시함. 물론 그 뉴런은 다음 훈련 스텝에서는 다시 사용 될 수 있음. 

![img](https://tva1.sinaimg.cn/large/008i3skNgy1grooamj9ezj31gg0t6qt4.jpg)

위의 사진처럼, 매 훈련 과정에서 x표가 쳐져있는 뉴런은 학습에 사용되지 않고, 남은 뉴런들로드만 학습을 진행하여 과적합을 방지하는 것임. 

Dropout은 training 때만 사용해야 하는데,  Test 단계에서 Dropout을 사용하여 일부 뉴런을 끄고 또 키는 것을 하면 같은 입력을 넣었을 때도 다른 결과를 배출할 수 있기 때문임. 또한 층마다 몇개의 뉴런을 살릴건지 결정하는 Keep_prob을 바꾸는 것도 가능한데 주로 과대적합의 우려가 더 큰 층에서는 다른 층보다 더 낮은 값의 keep_prob을 설정하곤 함.(더 큰 값의 드롭확률을 지정하는 것임.) 

Dropout의 단점으로는, 교차검증을 위해서는 더 많은 하이퍼파라미터가 생긴다는 점임. 또한 드롭아웃을 사용하게되면 Gradient checking 과정을 진행하기 어렵다는 단점이 있음. Gradient checking 과정이 뭐냐면, 간단하게 설명하면 우리가 계산한 gradient값이 맞는지를 확인하기 위해 forward로 미분값의 추정값(Grad_approx)를 구한 것과 실제 역전파를 통해 구한 Grad값을 비교하여, 오차가 10의 -7승 이하면 Grad값이 잘 구해졌다고 판단하는 것임. 

Dropout을 진행하게 되면 해당 신경망의 cost function을 제대로 구하는 것이 불가능하여, 해당 비용함수가 제대로 하강하고 있는지, (Gradient descent가 잘 진행되고 있는지..) 파악하는 것이 매우 어려워지게 됨. 따라서 gradient checking을 반드시 진행하고 넘어가야 하는 경우에는 학습 과정에서 dropout을 하지 않는 것이 좋음. 

추가적으로, dropout을 진행하게 되면 테스트를 진행 할 때 드롭되지 않은 뉴런이 훈련 때보다 더 많은 입력 뉴런과 연결되게 되는데, 이러한 점을 보상하기 위해서 훈련하고 나서 각 뉴련의 연결 가중치에 keep_prob을 곱해 주거나, 훈련을 진행하는 동안 각 뉴런의 출력을 keep_prob을 나눠 주어야 함. 

케라스에서는 keras.layers.Dropout 층을 사용하여 드롭아웃을 간단하게 구현 할 수 있으며, 해당 함수를 사용하면 드롭되지 않는 뉴런에 대해 keep_prob을 곱해주거나 하는 과정을 하지 않아도 됨. (함수에서 알아서 해 주는 듯 함)

### 11.4.3 몬테 카를로 드롭아웃

야린 갤과 주빈 가라마니는 2016년 논문에서 드롭아웃을 반드시 사용해야 만 하는 몇가지 이유를 소개했는데, 그 이유는 다음과 같음.

1. 해당 논문은 드롭아웃을 수학적으로 정의하여 드롭아웃 네트워크와 근사 베이즈 추론 사이에 깊은 관련성을 정립함.
2. 저자들은 훈련된 드롭아웃 모델을 재훈련하거나 전혀 수정하지 않고 성능을 크게 향상시킬 수 있는 몬테 카를로 드롭아웃 이라 불리는 강력한 기법을 소개하였으며, 또한 모델의 불확실성을 더 잘 측정 할 수 있고 코드 구현도 매우 간편함.

간단히 설명하면 MC드롭아웃은 드롭아웃 모델의 성능은 더 높여주고 더 정확한 불확실성 추정을 제공하는 모델이라고 할 수 있음.

### 11.4.4 맥스-노름 규제

맥스 노름 규제는 각각의 뉴런에 대해 입력의 연결 가중치 w에 대해, w의 L2노름이 r보다 작아지게끔 w를 제한하는 규제 방식임. 여기서 r은 하이퍼파라미터임 

이 맥스 노름 규제는 전체 손실 함수에 규제 term을 추가 하지 않고, 대신 일반적으로 매 훈련 스텝이 끝나고 w의 L2노름을 계산하고 만약 필요하다면 w의 스케일을 조정하게 됨. (w*(r/w의 L2노름)) r을 줄이게 되면 규제의 양이 증가하여 과대적합을 감소시키는데에 도움이 됨. 또한 맥스 노름 규제는 배치 정규화를 사용하지 않았을 때 불안정한 그래디언트 문제를 완화하는데에도 도움을 줄 수 있음. 

케라스에서는 케라스 레이어 안에 keras_constrains = keras.constraint.max_norm(1.)) 함수를 추가하여 간단하게 구현 할 수 있음. 